{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Patient Info Data for Medical Research\n",
    "\n",
    "This project involves extracting data from the Patient Info website, a platform offering medical advice, articles, and user discussions on various health conditions. The aim is to gather and process this data for applications such as medical research, patient sentiment analysis, and healthcare trend monitoring.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Scrape patient discussions, medical articles, and FAQs from specific categories on Patient Info (e.g., chronic illnesses, lifestyle, and medications).\n",
    "- **Data Processing**: Preprocess the gathered data, including cleaning text and standardizing formats for analysis.\n",
    "- **Data Storage**: Save the extracted data in a structured format like CSV, JSON, or a database for future use.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The main programming language for implementing the web scraping workflow.\n",
    "- **Beautiful Soup**: A library for parsing HTML and XML documents to extract relevant information.\n",
    "- **Selenium**: For handling dynamic web pages and automating the browser.\n",
    "- **Pandas**: For organizing, analyzing, and exporting the collected data into a structured format.\n",
    "\n",
    "#### Getting Started\n",
    "1. Set up the environment by installing the necessary Python libraries.\n",
    "2. Identify the target URLs based on categories of interest, such as \"Diabetes\" or \"Mental Health.\"\n",
    "3. Implement the scraping logic, including functions to retrieve article titles, discussion threads, and summaries while handling pagination and errors.\n",
    "4. Run the scraper to collect the data and ensure the process is monitored to address issues like CAPTCHA or IP blocking.\n",
    "5. Process and analyze the collected data, cleaning and organizing it using Pandas for further exploration.\n",
    "\n",
    "#### Ethical Considerations\n",
    "- Ensure compliance with the website's terms of use and avoid violating ethical guidelines.\n",
    "- Use the data responsibly, ensuring user privacy and data security.\n",
    "\n",
    "#### Conclusion\n",
    "This project provides a practical application of web scraping for healthcare research. By leveraging Patient Info's resources, researchers can gain valuable insights into patient experiences, emerging trends, and medical discussions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Health-Related Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# communities in patients Info are organize like index-letter grouping all topics starting with letter \n",
    "# for example https://patient.info/forums/index-b contain all topics starting with \"b\" like \"baby and infants\" ,  \"baclofen\" , \"backache\"  \n",
    "featuresIndexes=[f\"index-{chr(i)}\" for i in range(97,123)]\n",
    "topics=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapIndexedGroups(url):\n",
    "    print(f\"Actually Collecting topics with {url.split('index-')[1]}\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'}\n",
    "    request = Request(url, headers=headers)\n",
    "        \n",
    "    # Fetch page content\n",
    "    with urlopen(request) as response:\n",
    "        page_source = response.read()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    sourceElement=soup.find(\"table\",class_=\"table\")\n",
    "    tdElements=sourceElement.find_all(\"td\")\n",
    "    for element in tdElements:\n",
    "        topic={}\n",
    "        topic[\"topicLink\"]=\"https://patient.info\"+element.find(\"a\").get(\"href\")\n",
    "        topic[\"topicName\"]=element.find(\"a\").get_text()\n",
    "        topics.append(topic)\n",
    "for feature in featuresIndexes:\n",
    "    scrapIndexedGroups(f\"https://patient.info/forums/{feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "topics=pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.to_csv(\"../data/patientInfoTopics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each Topic , We Scrap Health-Related Posts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd \n",
    "topics=pd.read_csv(\"../data/patientInfoTopics.csv\")\n",
    "posts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodicSave(data,index):\n",
    "    print(f\"Periodic Save : {index}\")\n",
    "    data=pd.DataFrame(data)\n",
    "    data.to_csv(\"../data/patientInfosPosts2.csv\")\n",
    "def scrapTopicUrl(url,topic):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'}\n",
    "    request = Request(url, headers=headers)\n",
    "    print(f\"Navigating to {url}\")\n",
    "    # Fetch page content\n",
    "    with urlopen(request) as response:\n",
    "        page_source = response.read()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    elements=soup.find_all(\"article\",class_=\"post thread\")\n",
    "    for element in elements:\n",
    "        try:\n",
    "            postDetails=element.find(\"h3\",\"post__title\")\n",
    "            postAuthor=element.find(\"div\",\"post__actions\")\n",
    "            post={}\n",
    "            post[\"postTitle\"]=postDetails.find(\"a\").get_text()\n",
    "            post[\"postLink\"]=postDetails.find(\"a\").get(\"href\")\n",
    "            post[\"postType\"]=postDetails.find(\"a\").get(\"rel\")[0]\n",
    "            post[\"postTopic\"]=topic\n",
    "            post[\"postAuthor\"]=(postAuthor.find(\"a\").get_text().replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "            post[\"createdAt\"]=datetime.now()\n",
    "            post[\"commentsCount\"]=element.find(\"div\",class_=\"actions\").find(\"span\").get_text()\n",
    "            posts.append(post)\n",
    "            if(len(posts)%500==0):\n",
    "                periodicSave(posts,(len(posts))//500)\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        nextLink=(soup.find(\"a\",class_=\"reply__control reply-ctrl-last link\").get(\"href\"))\n",
    "\n",
    "        if(nextLink):\n",
    "            print(\"scrapped actual page , moving to the next one \")\n",
    "            scrapTopicUrl(nextLink,topic)\n",
    "    except :\n",
    "        print(\"finished actual topic , moving to the next topic\")\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(870,len(topics)):\n",
    "    topicName=topics.iloc[index][\"topicName\"]\n",
    "    topicLink=topics.iloc[index][\"topicLink\"]\n",
    "    scrapTopicUrl(topicLink,topicName)\n",
    "periodicSave(posts,len(posts))\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
