{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of Posts and Comments from Reddit\n",
    "\n",
    "In this project, we focus on extracting posts and comments from Reddit using web scraping techniques and interaction with the API. Reddit is a platform rich in user-generated content on various topics, making it a valuable resource for applications such as sentiment analysis, trend tracking, and topic modeling.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Retrieve posts and comments from specific subreddits based on thematic criteria (e.g., r/mentalhealth, r/fitness).\n",
    "- **Data Processing**: Clean and preprocess the extracted data to prepare it for analysis.\n",
    "- **Data Storage**: Store the collected data in a structured format, such as a CSV file or database, for later analysis.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The primary programming language for web scraping and interacting with the API.\n",
    "- **Requests**: A library for making HTTP requests, in case additional scraping is required.\n",
    "- **Pandas**: A data manipulation library for handling and analyzing the extracted data.\n",
    "\n",
    "#### Getting Started\n",
    "1. **Set Up the Environment**: Install necessary libraries using pip (`praw`, `requests`, `pandas`).\n",
    "2. **Obtain API Credentials**: Create a Reddit account and register an application to get API credentials (client ID, secret, and user agent).\n",
    "3. **Define the Extraction Logic**: Write functions to extract data from specific subreddits or threads based on keywords or categories.\n",
    "4. **Run the Scraper**: Launch the script and monitor the data collection process.\n",
    "5. **Analyze the Data**: Use Pandas to analyze the collected posts and comments for insights.\n",
    "\n",
    "#### Conclusion\n",
    "This project provides a hands-on introduction to using Reddit's API and analyzing data with Python, while also allowing manipulation of data from a dynamic online community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FBCE60;text-align:center;font-size:30px\"> Scraping Reddit's  Posts And Articles </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python310\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\python310\\lib\\site-packages (4.25.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\python310\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\python310\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\python310\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\python310\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python310\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\infokom\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python310\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\python310\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing BeautifulSoup4\n",
    "!pip install bs4\n",
    "\n",
    "# Installing Selenium\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Reddit's Health related Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup is used to parse HTML content.\n",
    "from datetime import datetime  # Allows handling and manipulating dates and times (not used in this script but potentially useful for adding timestamps).\n",
    "from urllib.request import urlopen, Request  # Allows sending HTTP requests and opening URLs.\n",
    "\n",
    "# Target URL containing information about Reddit health-related communities\n",
    "url = \"https://www.reddit.com/r/Health/wiki/communities/\"\n",
    "\n",
    "# List to store the collected information about subreddits\n",
    "topicList = []\n",
    "\n",
    "# Function to collect health-related subreddits\n",
    "def collectSubReddits():\n",
    "    # Creating an HTTP header to simulate a web browser. This is necessary to prevent the server from blocking the request.\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'\n",
    "    }\n",
    "    # Preparing the HTTP request with the target URL and headers\n",
    "    request = Request(url, headers=headers)\n",
    "    print(f\"Navigating to {url}\")  # Display the URL for user feedback\n",
    "\n",
    "    # Sending the request and retrieving the page content\n",
    "    with urlopen(request) as response:  # Opening the URL\n",
    "        page_source = response.read()  # Reading the HTML content of the page\n",
    "\n",
    "    # Creating a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Searching for the division containing the list of subreddits. The \"md wiki\" class seems to identify this area in the page.\n",
    "    divElement = soup.find(\"div\", class_=\"md wiki\")\n",
    "\n",
    "    # Extracting the unordered list (<ul>) containing the subreddits\n",
    "    listOfTopics = divElement.find(\"ul\")\n",
    "\n",
    "    # Iterating through each list item (<li>) element\n",
    "    for liElement in listOfTopics.find_all(\"li\"):\n",
    "        # Searching for the <a> (link) tag in each list item\n",
    "        aElement = liElement.find(\"a\")\n",
    "\n",
    "        # Creating a dictionary to store information about the subreddit\n",
    "        topic = {}\n",
    "        topic[\"topicName\"] = aElement.get_text()  # Extracting the subreddit name (text of the link)\n",
    "        topic[\"topicUrl\"] = \"https://old.reddit.com/\" + aElement.get(\"href\")  # Constructing the full URL of the subreddit\n",
    "\n",
    "        # Adding the dictionary to the list of topics\n",
    "        topicList.append(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to https://www.reddit.com/r/Health/wiki/communities/\n",
      "The data has been saved in the file '../data/healthRedditCommunities.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Library for handling data in the form of DataFrames.\n",
    "# Calling the function to collect subreddits\n",
    "collectSubReddits()\n",
    "\n",
    "# Converting the list of topics into a pandas DataFrame\n",
    "# Each dictionary in `topicList` becomes a row in the DataFrame.\n",
    "topicList = pd.DataFrame(topicList)\n",
    "\n",
    "# Saving the DataFrame as a CSV file\n",
    "# The path \"../data/healthRedditCommunities.csv\" can be modified based on the desired location.\n",
    "topicList.to_csv(\"../data/healthRedditCommunities.csv\", index=False)  # `index=False` excludes the pandas index in the CSV file.\n",
    "\n",
    "# Displaying a confirmation message about the save\n",
    "print(\"The data has been saved in the file '../data/healthRedditCommunities.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
