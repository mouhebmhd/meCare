{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of Posts and Comments from Reddit\n",
    "\n",
    "In this project, we focus on extracting posts and comments from Reddit using web scraping techniques and interaction with the API. Reddit is a platform rich in user-generated content on various topics, making it a valuable resource for applications such as sentiment analysis, trend tracking, and topic modeling.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Retrieve posts and comments from specific subreddits based on thematic criteria (e.g., r/mentalhealth, r/fitness).\n",
    "- **Data Processing**: Clean and preprocess the extracted data to prepare it for analysis.\n",
    "- **Data Storage**: Store the collected data in a structured format, such as a CSV file or database, for later analysis.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The primary programming language for web scraping and interacting with the API.\n",
    "- **Requests**: A library for making HTTP requests, in case additional scraping is required.\n",
    "- **Pandas**: A data manipulation library for handling and analyzing the extracted data.\n",
    "\n",
    "#### Getting Started\n",
    "1. **Set Up the Environment**: Install necessary libraries using pip (`praw`, `requests`, `pandas`).\n",
    "2. **Obtain API Credentials**: Create a Reddit account and register an application to get API credentials (client ID, secret, and user agent).\n",
    "3. **Define the Extraction Logic**: Write functions to extract data from specific subreddits or threads based on keywords or categories.\n",
    "4. **Run the Scraper**: Launch the script and monitor the data collection process.\n",
    "5. **Analyze the Data**: Use Pandas to analyze the collected posts and comments for insights.\n",
    "\n",
    "#### Conclusion\n",
    "This project provides a hands-on introduction to using Reddit's API and analyzing data with Python, while also allowing manipulation of data from a dynamic online community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FBCE60;text-align:center;font-size:30px\"> Scraping Reddit's  Posts And Articles </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python310\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\python310\\lib\\site-packages (4.25.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\python310\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\python310\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\python310\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\python310\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python310\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\infokom\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python310\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\python310\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing BeautifulSoup4\n",
    "!pip install bs4\n",
    "\n",
    "# Installing Selenium\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Reddit's Health related Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'\n",
    "}\n",
    "\n",
    "async def fetch_page(session, postUrl):\n",
    "    \"\"\"Fetch the page content asynchronously.\"\"\"\n",
    "    try:\n",
    "        async with session.get(postUrl, headers=headers, timeout=10) as response:\n",
    "            if response.status == 200:\n",
    "                print(f\"Navigating to {postUrl}\")\n",
    "                return await response.text()\n",
    "            else:\n",
    "                print(f\"Failed to fetch {postUrl}: HTTP {response.status}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {postUrl}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def collect_subreddit_post_text(postUrl):\n",
    "    \"\"\"Collect the text body of a subreddit post asynchronously.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        page_source = await fetch_page(session, postUrl)\n",
    "        if page_source:\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            response_element = soup.find(\"div\", slot=\"text-body\")\n",
    "            if response_element:\n",
    "                paragraphs = [p.get_text(strip=True) for p in response_element.find_all(\"p\")]\n",
    "                return \" \".join(paragraphs)\n",
    "            else:\n",
    "                print(f\"No content found at {postUrl}\")\n",
    "                return \"\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "async def getPostText(post_url):\n",
    "    return await collect_subreddit_post_text(post_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd \n",
    "# Current timestamp\n",
    "now = datetime.now(timezone.utc)\n",
    "\n",
    "# function to periodically save collected data \n",
    "def periodicSave(data,topic):\n",
    "    data=pd.DataFrame(data)\n",
    "    data.to_csv(f\"../data/healthRedditPosts/redditPosts{topic}.csv\")\n",
    "    print(\"Periodic Save is done , Total of saved posts is  \" , len(data))\n",
    "# Function to collect health-related subreddits\n",
    "def collectSubRedditsPosts(url,topic,posts):\n",
    "    time.sleep(5)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'}\n",
    "    request = Request(url, headers=headers)\n",
    "\n",
    "    # Open the URL and read the page content\n",
    "    with urlopen(request) as response:\n",
    "        page_source = response.read()\n",
    "    print(f\"Navigating to {url}\")\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find and process posts\n",
    "    \n",
    "    posts_elements = soup.find_all(\"div\",class_=\"thing\")\n",
    "    for post_element in posts_elements:\n",
    "        try:\n",
    "            post = {\n",
    "                \"authorName\": post_element.get(\"data-author\"),\n",
    "                \"authorId\": post_element.get(\"data-author-fullname\"),\n",
    "                \"commentCount\": post_element.get(\"data-comments-count\"),\n",
    "                \"commentsLink\": post_element.get(\"data-url\"),\n",
    "                \"createdAt\": post_element.get(\"data-timestamp\"),\n",
    "                \"postId\": post_element.get(\"id\"),\n",
    "                \"postTitle\": post_element.find(\"a\",class_=\"title may-blank\").get_text(),\n",
    "                \"subredditName\": post_element.get(\"data-subreddit-prefixed\"),\n",
    "                \"collectedAt\": now.strftime(\"%Y-%m-%dT%H:%M:%S.\") + str(now.microsecond).ljust(6, '0') + \"+0000\",\n",
    "                \"interactionCategory\": post_element.find(\"span\", class_=\"flairrichtext\").get_text(\"title\")\n",
    "                if post_element.find(\"span\", class_=\"flairrichtext\") else \"N/A\",\n",
    "            }\n",
    "            print(post)\n",
    "            posts.append(post)\n",
    "            if(len(posts)%100==0):\n",
    "                periodicSave(posts,topic)\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        nextButton=soup.find(\"a\",rel=\"nofollow next\").get(\"href\")\n",
    "        if(nextButton):\n",
    "            collectSubRedditsPosts(nextButton,topic,posts)\n",
    "    except :\n",
    "        print(\"no next button found . Stoppig Scroll \")\n",
    "        periodicSave(posts,topic)\n",
    "        return \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "topicsList=pd.read_csv(\"../data/healthRedditPosts/healthRedditCommunities.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time\n",
    "collectedPosts=[]\n",
    "\n",
    "for index in range(len(topicsList)):\n",
    "    topic=topicsList.iloc[index]\n",
    "    topicName=topic[\"topicName\"]\n",
    "    baseUrl=topic[\"topicUrl\"]\n",
    "    extendedUrls = [\n",
    "            baseUrl,\n",
    "            baseUrl + \"/new/\", \n",
    "            baseUrl + \"/rising/\",  \n",
    "            baseUrl + \"/controversial/\",  \n",
    "            baseUrl + \"/controversial/?sort=controversial&t=all\",  \n",
    "            baseUrl + \"/controversial/?sort=controversial&t=month\", \n",
    "            baseUrl + \"/controversial/?sort=controversial&t=year\",  \n",
    "            baseUrl + \"/controversial/?sort=controversial&t=week\",  \n",
    "            baseUrl + \"/controversial/?sort=controversial&t=hour\",  \n",
    "            baseUrl + \"/top/\",  \n",
    "            baseUrl + \"/top/?sort=controversial&t=all\",  \n",
    "            baseUrl + \"/top/?sort=controversial&t=month\",\n",
    "            baseUrl + \"/top/?sort=controversial&t=year\",  \n",
    "            baseUrl + \"/top/?sort=controversial&t=week\",  \n",
    "            baseUrl + \"/top/?sort=controversial&t=hour\" \n",
    "        ]\n",
    "    for topicUrl in extendedUrls:\n",
    "        collectedPosts=collectSubRedditsPosts(topicUrl,topicName,[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to https://reddit.com/r/breastcancer/comments/1focbhi/signing_off_best_wishes_to_all/\n",
      "Post number 0 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/ynowi8/i_need_advice/\n",
      "Post number 1 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fqrkxe/dame_maggie_smith/\n",
      "Post number 2 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fgjbkm/beating_the_odds/\n",
      "Post number 3 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dbb3cq/lost_my_wife/\n",
      "Post number 4 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gevsrb/remind_me_to_never_post_outside_of_this_sub_again/\n",
      "Post number 5 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gnwmli/bc_treatments_are_all_terrible_and_im_not/\n",
      "Post number 6 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/12sbun0/cancer_free/\n",
      "Post number 7 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/11o7x3h/its_over_i_did_it/\n",
      "Post number 8 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1g46i88/humans/\n",
      "Post number 9 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1g56xh4/cancer_has_exited_the_building/\n",
      "Post number 10 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1grhgv4/all_done/\n",
      "Post number 11 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fpmh8g/15_year_cancerversary_breast_cancer_yay/\n",
      "Post number 12 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1e8tp08/were_not_like_other_girls/\n",
      "Post number 13 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/18jrx9x/played_the_cancer_card_hard_today/\n",
      "Post number 14 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gwisae/one_year_later/\n",
      "Post number 15 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1foe5zn/out_of_the_mouths_of_babes/\n",
      "Post number 16 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1czflnd/my_husband_sleeps_in_my_bras/\n",
      "Post number 17 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dp6ljd/its_been_a_year_i_said_it_was_impossible_but_here/\n",
      "Post number 18 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1go9spv/asking_for_prayers/\n",
      "Post number 19 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1buryfb/whats_one_thing_youve_learned_from_having_breast/\n",
      "Post number 20 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1eba74z/it_gets_better/\n",
      "Post number 21 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1bl7caw/kate_middleton/\n",
      "Post number 22 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1e32bpt/rip_shannon/\n",
      "Post number 23 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/18du8dw/rant_and_shes_fine/\n",
      "Post number 24 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1evenzv/long_term_survival/\n",
      "Post number 25 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1c5dvod/just_had_my_last_radiation_session_done_with/\n",
      "Post number 26 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dcwow2/im_lucky_i_got_breast_cancer_who_actually_says/\n",
      "Post number 27 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1bdw7x8/i_finally_got_the_call_im_free/\n",
      "Post number 28 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gfkt5y/anyone_else_just_want_to_be_left_alone_to_be_a/\n",
      "Post number 29 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gkwi75/scared/\n",
      "Post number 30 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dzypns/today_i_rang_the_bell/\n",
      "Post number 31 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/15d1lr2/shes_gone/\n",
      "Post number 32 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fa25de/almost_4_years_from_diagnosisthings_are_good_keep/\n",
      "Post number 33 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/17sb4f5/proof_that_this_can_strike_anyone/\n",
      "Post number 34 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gwzmpl/no_one_warned_me_about_the_breast_mri_for_laughs/\n",
      "Post number 35 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h00nh2/rip_to_a_great_rack/\n",
      "Post number 36 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fpawcz/last_day_of_chemo/\n",
      "Post number 37 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1bfebep/the_afterward_no_one_talks_about/\n",
      "Post number 38 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1f2rl8j/cancer_free/\n",
      "Post number 39 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dumrhy/update_to_dumped/\n",
      "Post number 40 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1cp2386/i_forgot_i_didnt_have_hair/\n",
      "Post number 41 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/171sf0z/who_else_is_pissed_about_false_information_that/\n",
      "Post number 42 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/114zyt4/new_rule_announcement/\n",
      "Post number 43 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1d74oi7/cancer_free/\n",
      "Post number 44 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1byvm4t/a_surprising_and_most_welcomed_side_effect_of/\n",
      "Post number 45 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fu20ke/my_sister_is_dying/\n",
      "Post number 46 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1ehwdyc/omgim_such_a_dork/\n",
      "Post number 47 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fy7uzu/psa_to_people_in_life_breast_cancer_is_not_easy/\n",
      "Post number 48 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1f9jkwb/some_hope_for_you_from_the_farther_shore/\n",
      "Post number 49 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/rxpvol/stage_iv_triple_negative_metastatic_breast_cancer/\n",
      "Post number 50 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gi7c6x/people_suck/\n",
      "Post number 51 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h4japg/people_have_surprised_me/\n",
      "Post number 52 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dkireu/i_did_it_i_had_my_last_radiation_treatment/\n",
      "Post number 53 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1g64an9/cancer_free/\n",
      "Post number 54 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1dz58mm/i_love_oncology_nurses_but/\n",
      "Post number 55 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/195bqxh/big_pharma_doesnt_want_cancer_cured/\n",
      "Post number 56 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h5wiw2/good_news/\n",
      "Post number 57 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1frf3by/an_update_i_didnt_think_id_be_posting/\n",
      "Post number 58 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h570fo/i_finished_my_last_chemotherapy_session/\n",
      "Post number 59 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fkyria/pregnancy_and_cancer_tfmr/\n",
      "Post number 60 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fsc3zh/i_am_so_scared/\n",
      "Post number 61 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/150b0o0/male_breast_cancer_37y/\n",
      "Post number 62 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/ycoyxn/i_am_finished/\n",
      "Post number 63 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1chr1os/2_years_after_diagnosis/\n",
      "Post number 64 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1ece44j/yesterday_i_rang_that_damn_bell/\n",
      "Post number 65 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/17m1u3g/i_got_my_diagnosis_11_years_ago_today/\n",
      "Post number 66 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gh4avg/just_got_diagnosed_male_46_joy/\n",
      "Post number 67 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1c3f1zw/stage_iv/\n",
      "Post number 68 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1arly6t/my_mo_was_downright_giddy_yesterday/\n",
      "Post number 69 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1ct93wf/as_a_tnbc_girl_i_am_celebrating_big_milestone/\n",
      "Post number 70 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/xbll3z/i_made_it/\n",
      "Post number 71 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gec3g0/a_good_day/\n",
      "Post number 72 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/15c0kfa/our_doctor_just_made_the_call_to_stop_treatment/\n",
      "Post number 73 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1eymuhi/today_i_turn_30/\n",
      "Post number 74 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1exqw20/final_chemo_today/\n",
      "Post number 75 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1910tun/the_time_it_takes_to_get_to_treatment_after/\n",
      "Post number 76 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/15rahr9/i_summited/\n",
      "Post number 77 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gb1tmq/one_year_post_surgerygood_news/\n",
      "Post number 78 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1fh9pqn/cheating_husband/\n",
      "Post number 79 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1bimfpk/success/\n",
      "Post number 80 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1eof3qr/apparently_my_bald_head_is_ugly_scary_and/\n",
      "Post number 81 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1cc2fup/my_mothers_longevity/\n",
      "Post number 82 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h3782n/looks_like_the_breast_cancer_has_spread_to_my/\n",
      "Post number 83 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1d8wj4b/you_cut_your_hair/\n",
      "Post number 84 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gg2om4/good_news/\n",
      "Post number 85 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1efbwhf/8_years_ned_7_years_on_tamoxifen_out_of_10/\n",
      "Post number 86 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/18q98br/merry_christmas_to_some_of_the_strongest_people_i/\n",
      "Post number 87 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/159bt60/cancer_personality/\n",
      "Post number 88 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1cn9qlz/real_life_friends_premature_diagnosis_announcement/\n",
      "Post number 89 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1h49mnj/just_wanna_say/\n",
      "Post number 90 is done.\n",
      "Navigating to https://reddit.com/r/breastcancer/comments/1gqsj6d/anyone_else_just_cant_be_bothered_to_eat_well_or/\n",
      "Post number 91 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/v1e7a0/the_last_day_of_radiation_im_done/: HTTP 429\n",
      "Post number 92 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/18j5b1c/holidays_and_dying/: HTTP 429\n",
      "Post number 93 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1eqi950/celebrating/: HTTP 429\n",
      "Post number 94 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1dgpim3/who_else_was_diagnosed_not_long_after_an/: HTTP 429\n",
      "Post number 95 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1at89n9/i_quit/: HTTP 429\n",
      "Post number 96 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/15txqhy/done_with_chemo_done_with_surgery/: HTTP 429\n",
      "Post number 97 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1gjiiq9/more_than_i_can_handle/: HTTP 429\n",
      "Post number 98 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1ewhal4/the_chemo_is_working/: HTTP 429\n",
      "Post number 99 is done.\n",
      "Failed to fetch https://reddit.com/r/breastcancer/comments/1e3ji5w/incredible_vaginal_atrophy_progress_and_whats/: HTTP 429\n",
      "Post number 100 is done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"../data/healthRedditPosts/redditPostsBipolarReddit.csv\")\n",
    "\n",
    "async def process_posts(dataset):\n",
    "    \"\"\"Process posts asynchronously.\"\"\"\n",
    "    for i in range(0, len(dataset)):  # Ensure we don't exceed the dataset length\n",
    "        post = dataset.iloc[i]\n",
    "        postLink = post[\"commentsLink\"]\n",
    "        postText = await getPostText(\"https://reddit.com\" + postLink)\n",
    "        dataset.loc[i, \"postText\"] = postText\n",
    "        print(f\"Post number {i} is done.\")\n",
    "    return dataset\n",
    "\n",
    "# Run the asynchronous loop\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    updated_dataset = await process_posts(dataset)\n",
    "    # Save the updated dataset after processing\n",
    "    updated_dataset.to_csv(\"../data/healthRedditPosts/redditPostsBreast_Cancer_with_text.csv\", index=False)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
