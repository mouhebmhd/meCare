{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of Posts and Comments from Reddit\n",
    "\n",
    "In this project, we focus on extracting posts and comments from Reddit using web scraping techniques and interaction with the API. Reddit is a platform rich in user-generated content on various topics, making it a valuable resource for applications such as sentiment analysis, trend tracking, and topic modeling.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Retrieve posts and comments from specific subreddits based on thematic criteria (e.g., r/mentalhealth, r/fitness).\n",
    "- **Data Processing**: Clean and preprocess the extracted data to prepare it for analysis.\n",
    "- **Data Storage**: Store the collected data in a structured format, such as a CSV file or database, for later analysis.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The primary programming language for web scraping and interacting with the API.\n",
    "- **Requests**: A library for making HTTP requests, in case additional scraping is required.\n",
    "- **Pandas**: A data manipulation library for handling and analyzing the extracted data.\n",
    "\n",
    "#### Getting Started\n",
    "1. **Set Up the Environment**: Install necessary libraries using pip (`praw`, `requests`, `pandas`).\n",
    "2. **Obtain API Credentials**: Create a Reddit account and register an application to get API credentials (client ID, secret, and user agent).\n",
    "3. **Define the Extraction Logic**: Write functions to extract data from specific subreddits or threads based on keywords or categories.\n",
    "4. **Run the Scraper**: Launch the script and monitor the data collection process.\n",
    "5. **Analyze the Data**: Use Pandas to analyze the collected posts and comments for insights.\n",
    "\n",
    "#### Conclusion\n",
    "This project provides a hands-on introduction to using Reddit's API and analyzing data with Python, while also allowing manipulation of data from a dynamic online community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FBCE60;text-align:center;font-size:30px\"> Scraping Reddit's  Posts And Articles </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python310\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\python310\\lib\\site-packages (4.25.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\python310\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\python310\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\python310\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\python310\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python310\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\infokom\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python310\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\python310\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing BeautifulSoup4\n",
    "!pip install bs4\n",
    "\n",
    "# Installing Selenium\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Reddit's Health related Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup is used to parse HTML content.\n",
    "from datetime import datetime  # Allows handling and manipulating dates and times (not used in this script but potentially useful for adding timestamps).\n",
    "from urllib.request import urlopen, Request  # Allows sending HTTP requests and opening URLs.\n",
    "from datetime import datetime, timezone\n",
    "now = datetime.now(timezone.utc)\n",
    "# List to store the collected information about subreddits and posts\n",
    "posts = []\n",
    "\n",
    "# Function to collect health-related subreddits\n",
    "def collectSubRedditsPosts(url):\n",
    "\n",
    "    # Creating an HTTP header to simulate a web browser. This is necessary to prevent the server from blocking the request.\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.137 Safari/537.36'\n",
    "    }\n",
    "    # Preparing the HTTP request with the target URL and headers\n",
    "    request = Request(url, headers=headers)\n",
    "    print(f\"Navigating to {url}\")  # Display the URL for user feedback\n",
    "\n",
    "    # Sending the request and retrieving the page content\n",
    "    with urlopen(request) as response:  # Opening the URL\n",
    "        page_source = response.read()  # Reading the HTML content of the page\n",
    "\n",
    "    # Creating a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    postsElements=soup.find_all(\"shreddit-post\")\n",
    "    for postsElement in postsElements:\n",
    "        post={}\n",
    "        post[\"authorName\"]=postsElement.get(\"author\")\n",
    "        post[\"authorId\"]=postsElement.get(\"author-id\")\n",
    "        post[\"commentCount\"]=postsElement.get(\"comment-count\")\n",
    "        post[\"commentsLink\"]=postsElement.get(\"content-href\")\n",
    "        post[\"createdAt\"]=postsElement.get(\"created-timestamp\")\n",
    "        post[\"postId\"]=postsElement.get(\"id\")\n",
    "        post[\"postTitle\"]=postsElement.get(\"post-title\")\n",
    "        post[\"subredditName\"]=postsElement.get(\"subreddit-prefixed-name\")\n",
    "        post[\"createdAt\"]=postsElement.get(\"created-timestamp\")\n",
    "        post[\"collectedAt\"] = now.strftime(\"%Y-%m-%dT%H:%M:%S.\") + str(now.microsecond).ljust(6, '0') + \"+0000\"\n",
    "        post[\"interactionCategory\"]=soup.find(\"div\",class_=\"flair-content\").get_text().replace(\"\\n\", \"\").strip()\n",
    "        print(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to https://www.reddit.com/r/ADHD\n",
      "{'authorName': 'kswildcatmom', 'authorId': 't2_13byht', 'commentCount': '11', 'commentsLink': 'https://www.reddit.com/r/ADHD/comments/1h3yg30/my_normal_husband/', 'createdAt': '2024-12-01T07:59:29.479000+0000', 'postId': 't3_1h3yg30', 'postTitle': 'My normal husband', 'subredditName': 'r/ADHD', 'collectedAt': '2024-12-01T09:24:15.255182+0000', 'interactionCategory': 'Tips/Suggestions'}\n",
      "{'authorName': 'Paranoia_King', 'authorId': 't2_6jipcd7v', 'commentCount': '78', 'commentsLink': 'https://www.reddit.com/r/ADHD/comments/1h3ebqm/a_friendly_reminder_for_my_fellow_adhders_start/', 'createdAt': '2024-11-30T14:59:51.150000+0000', 'postId': 't3_1h3ebqm', 'postTitle': \"A friendly reminder for my fellow ADHD'ers, start buying your christmas gifts NOW\", 'subredditName': 'r/ADHD', 'collectedAt': '2024-12-01T09:24:15.255182+0000', 'interactionCategory': 'Tips/Suggestions'}\n",
      "{'authorName': 'check1232', 'authorId': 't2_19y9p7vbrf', 'commentCount': '36', 'commentsLink': 'https://www.reddit.com/r/ADHD/comments/1h3vl2o/so_what_happens_to_people_who_cant_handle_a_job/', 'createdAt': '2024-12-01T04:53:53.272000+0000', 'postId': 't3_1h3vl2o', 'postTitle': 'So what happens to people who can’t handle a job?', 'subredditName': 'r/ADHD', 'collectedAt': '2024-12-01T09:24:15.255182+0000', 'interactionCategory': 'Tips/Suggestions'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "topicsList=pd.read_csv(\"../data/healthRedditCommunities.csv\")\n",
    "for index in range(0,1):\n",
    "    topic=topicsList.iloc[index]\n",
    "    topicName=topic[\"topicName\"]\n",
    "    topicUrl=topic[\"topicUrl\"]\n",
    "    collectSubRedditsPosts(topicUrl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
