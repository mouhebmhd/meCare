{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Reddit Posts and Comments\n",
    "\n",
    "In this project, we focus on extracting posts and comments from Reddit using web scraping and API interaction techniques. Reddit is a platform with a wealth of user-generated content on various topics, making it an excellent resource for applications such as sentiment analysis, trend monitoring, and topic modeling.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Retrieve posts and comments from specific subreddits based on topic criteria (e.g., r/mentalhealth, r/fitness).\n",
    "- **Data Processing**: Clean and preprocess the extracted data to prepare it for analysis.\n",
    "- **Data Storage**: Store the scraped data in a structured format, such as CSV or a database, for further analysis.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The primary programming language for web scraping and API interaction..\n",
    "- **Requests**: A library for making HTTP requests if additional scraping is needed.\n",
    "- **Pandas**: A data manipulation library to handle and analyze the scraped data.\n",
    "\n",
    "#### Getting Started\n",
    "1. **Set Up the Environment**: Install the necessary libraries using pip (`praw`, `requests`, `pandas`).\n",
    "2. **Obtain API Credentials**: Create a Reddit account and register an application to get API credentials (client ID, secret, and user agent).\n",
    "3. **Define Data Extraction Logic**: Write functions to extract data from specific subreddits or threads based on keywords or categories.\n",
    "4. **Run the Scraper**: Execute the script and monitor the data collection process.\n",
    "5. **Analyze the Data**: Use Pandas to analyze the collected posts and comments for insights.\n",
    "\n",
    "#### Conclusion\n",
    "This project serves as a practical introduction to using Reddit's API and data analysis with Python, providing valuable experience in handling real-world data from a vibrant online community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FBCE60;text-align:center;font-size:30px\"> Scraping Reddit's  Posts And Articles </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Health Boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FBCE60;text-align:center;font-size:20px\"> Searching for Reddit's health related topics  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INFOKOM\\AppData\\Local\\Temp\\ipykernel_76040\\4287673666.py:2: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  postDataset=pd.read_csv(\"../data/redditPostsDataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "postDataset=pd.read_csv(\"../data/redditPostsDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the first NaN value is: 45280\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any NaN values\n",
    "if postDataset[\"postText\"].isna().sum() > 0:\n",
    "    # Find the index of the first NaN value\n",
    "    first_na_index = postDataset[\"postText\"].isna().idxmax()\n",
    "    print(f\"The index of the first NaN value is: {first_na_index}\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the 'postText' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver # type: ignore\n",
    "from selenium.webdriver.common.by import By # type: ignore\n",
    "from bs4 import BeautifulSoup # type: ignore\n",
    "from selenium.webdriver.common.by import By # type: ignore\n",
    "from selenium.webdriver.support.ui import WebDriverWait# type: ignore\n",
    "from selenium.webdriver.support import expected_conditions as EC# type: ignore\n",
    "def cleanText(inputText):\n",
    "    # Remove line returns and extra spaces\n",
    "    cleaned_text = \" \".join(inputText.split())\n",
    "    return cleaned_text\n",
    "def scrape_page(url):\n",
    "\n",
    "    # Set up the Selenium WebDriver\n",
    "    driver = webdriver.Chrome()  # Ensure you have the correct WebDriver\n",
    "    driver.get(url)    \n",
    "\n",
    "    # Use WebDriverWait to wait until the required element is visible\n",
    "    wait = WebDriverWait(driver, 10)  # Wait up to 10 seconds\n",
    "    new_content = wait.until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div[slot='text-body'] p\"))\n",
    "    )\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    new_content = soup.find(\"div\",slot=\"text-body\").find(\"p\")\n",
    "    try:\n",
    "        return new_content.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post: {e}\")\n",
    "\n",
    "    return new_content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does any work at grocery store upfront doing bagging, sweeping, carts, clean up spills etc., have autism/Asperger syndrome and has had a problem with a cashier you like, you gossiped to them about other coworkers and they where saying some rude things then the next week your manager brought you into the office to have a talk to you about gossiping to the cashier you like and told you that he or she heard you gossiping and told you to stop talking to that cashier and you also got told you where doing some other things wrong and the manager told you if you keep this up you could get get fired and lastly you got disappointed and sad because you can only talk to the cashier about work and you where doing some other things that could get you fired?\n",
      "Possible autism\n",
      "I am not a huge fan of all the depressing posts, although i get why people make them. I totally do.\n"
     ]
    }
   ],
   "source": [
    "for i in range(first_na_index,len(postDataset)):\n",
    "    try:\n",
    "        topic=postDataset.iloc[i]\n",
    "        link=topic[\"commentsLink\"]\n",
    "        scrappingResult=scrape_page(link)\n",
    "        print(cleanText(scrappingResult))\n",
    "        postDataset.loc[i,\"postText\"]=cleanText(scrappingResult)\n",
    "    except:\n",
    "        postDataset.loc[i,\"postText\"]=\"an error has occured while scrapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "postDataset.to_csv(\"../data/redditPostsDataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
