{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Health Board's Posts and Articles\n",
    "\n",
    "In this project, we focus on extracting posts and articles from Health Board forums using web scraping techniques. Health Board is a platform with a wealth of user-generated content on various health topics, making it an excellent resource for applications such as sentiment analysis, trend monitoring, and topic modeling.\n",
    "\n",
    "#### Objectives\n",
    "- **Data Collection**: Retrieve posts and articles from specific health-related categories on Health Board based on topic criteria (e.g., mental health, fitness).\n",
    "- **Data Processing**: Clean and preprocess the extracted data to prepare it for analysis.\n",
    "- **Data Storage**: Store the scraped data in a structured format, such as CSV or a database, for further analysis.\n",
    "\n",
    "#### Tools and Technologies\n",
    "- **Python**: The primary programming language for web scraping.\n",
    "- **Beautiful Soup**: A library for parsing HTML and extracting data.\n",
    "- **Requests**: A library for making HTTP requests to access web pages.\n",
    "- **Pandas**: A data manipulation library to handle and analyze the scraped data.\n",
    "\n",
    "#### Getting Started\n",
    "1. **Set Up the Environment**: Install the necessary libraries using pip.\n",
    "2. **Define Scraping Logic**: Write functions to scrape data from specific health categories on Health Board.\n",
    "3. **Run the Scraper**: Execute the scraping script and monitor the data collection process.\n",
    "4. **Analyze the Data**: Use Pandas to analyze the collected posts and articles for insights.\n",
    "\n",
    "#### Conclusion\n",
    "This project serves as a practical introduction to web scraping and data analysis using Python, providing valuable experience in handling real-world data from an online health community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FE4406;text-align:center;font-size:30px\"> Scraping Health board's  Posts And Articles </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python310\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\python310\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\python310\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\python310\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\python310\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\python310\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python310\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python310\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\infokom\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python310\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\python310\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\python310\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Health Boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FFC107;text-align:left;font-size:20px\"> Searching for Health Board's health related topics  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Ensure you have the correct WebDriver\n",
    "communities = []\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.healthboards.com/boards/hbcategory.php'\n",
    "driver.get(url)\n",
    "\n",
    "# Allow the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "def scrape_current_page():\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all posts in the page (use the common class or structure to target posts)\n",
    "    communities_elements = soup.find_all('b')\n",
    "\n",
    "    new_content_found = False\n",
    "    titlesList=soup.find_all(\"font\")\n",
    "    \n",
    "    for community_element in titlesList:\n",
    "        try:\n",
    "            category=(community_element.contents[0])\n",
    "            aElement=((community_element.parent.parent.parent))\n",
    "            for element in aElement.next_siblings:\n",
    "                if(str(element).startswith(\"<li>\")):\n",
    "                    topicLink=(element.a[\"href\"])\n",
    "                    topicName=(element.a.contents[0])\n",
    "                    # Ensure the list exists\n",
    "                    element={}\n",
    "                    element[\"topicName\"]=topicName\n",
    "                    element[\"topicLink\"]=topicLink\n",
    "                    if(element not in communities):\n",
    "                        communities.append(element)  \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post: {e}\")\n",
    "\n",
    "    return new_content_found\n",
    "# Scrape content from the current page\n",
    "content_found = scrape_current_page()\n",
    "# Close the driver when done\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topicName</th>\n",
       "      <th>topicLink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addison's Disease</td>\n",
       "      <td>https://www.healthboards.com/boards/forumdispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arthritis</td>\n",
       "      <td>https://www.healthboards.com/boards/forumdispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autoimmune Disorders</td>\n",
       "      <td>https://www.healthboards.com/boards/forumdispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chronic Fatigue</td>\n",
       "      <td>https://www.healthboards.com/boards/forumdispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>https://www.healthboards.com/boards/forumdispl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topicName                                          topicLink\n",
       "0     Addison's Disease  https://www.healthboards.com/boards/forumdispl...\n",
       "1             Arthritis  https://www.healthboards.com/boards/forumdispl...\n",
       "2  Autoimmune Disorders  https://www.healthboards.com/boards/forumdispl...\n",
       "3       Chronic Fatigue  https://www.healthboards.com/boards/forumdispl...\n",
       "4              Diabetes  https://www.healthboards.com/boards/forumdispl..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the topicLinks list to a dataset \n",
    "import pandas as pd\n",
    "communities=pd.DataFrame(communities)\n",
    "communities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities=communities.to_csv(\"../data/healthBoardsTopics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping topics from the topics list \n",
    "import pandas as pd \n",
    "topicsList=pd.read_csv(\"../data/healthBoardsTopics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsNotDone=pd.read_csv(\"../data/healthBordsPosts.csv\")[:8468][\"topicTag\"]\n",
    "topics=[]\n",
    "topicsNotDone=[element.replace(\" \",\"\") for element in list(set(list(topicsNotDone)))]\n",
    "indexes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(pd.read_csv(\"../data/healthBoardsTopics.csv\"))):\n",
    "    if(topicsList.iloc[index][\"topicName\"].replace(\" \",\"\") in  topicsNotDone):\n",
    "        indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "posts = []\n",
    "\n",
    "def get_total_pages(soup):\n",
    "    page_info = soup.find(\"td\", class_=\"vbmenu_control\", style=\"font-weight:normal\")\n",
    "    if page_info:\n",
    "        match = re.search(r\"Page \\d+ of (\\d+)\", page_info.text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return 1\n",
    "\n",
    "def scrape_current_page(url, topicTag):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"threadslist\")))\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    tableElement = soup.find(\"table\", id=\"threadslist\")\n",
    "    if tableElement:\n",
    "        trElements = tableElement.find_all(\"tr\")\n",
    "        for i in range(5, len(trElements)):\n",
    "            try:\n",
    "                content_found = trElements[i]\n",
    "                author = content_found.find(\"div\", class_=\"smallfont\").contents[0].strip().replace(\" \", \"\")\n",
    "                if author != \"Administrator\":\n",
    "                    element = {\n",
    "                        \"commentsLink\": content_found.find(\"a\", id=re.compile(\".*thread_title.*\"))['href'],\n",
    "                        \"postTitle\": content_found.find(\"a\", id=re.compile(\".*thread_title.*\")).contents[0],\n",
    "                        \"authorId\": author,\n",
    "                        \"postId\": content_found.find(\"a\", id=re.compile(\".*thread_title.*\"))[\"id\"].replace(\"thread_title_\", \"\"),\n",
    "                        \"commentsCount\": content_found.find_all(\"td\", class_=\"alt2\")[1][\"title\"].split(\"Replies:\")[1].split(\",\")[0].strip(),\n",
    "                        \"createdAt\": content_found.find_all(\"div\", class_=\"smallfont\")[1].contents[0].strip().replace(\" \", \"\") + \" \" + content_found.find(\"span\", class_=\"time\").contents[0].strip().replace(\" \", \"\"),\n",
    "                        \"collectedAt\": datetime.now(),\n",
    "                        \"topicTag\": topicTag\n",
    "                    }\n",
    "                    print(element)\n",
    "                    posts.append(element)\n",
    "\n",
    "                    if len(posts) % 20 == 0:\n",
    "                        return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post: {e}\")\n",
    "                continue\n",
    "    return False\n",
    "\n",
    "for  index in indexes[2:]:\n",
    "    topic_url = topicsList.loc[index, \"topicLink\"]\n",
    "    topic_tag = topicsList.loc[index, \"topicName\"]\n",
    "    \n",
    "    driver.get(topic_url)\n",
    "    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, \"threadslist\")))\n",
    "    first_page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    total_pages = get_total_pages(first_page_soup)\n",
    "    print(\"The number of total pages is \", total_pages)\n",
    "\n",
    "    page_number = 1\n",
    "    while page_number <= total_pages:\n",
    "        if page_number == 1:\n",
    "            current_page_url = topic_url\n",
    "        else:\n",
    "            current_page_url = f\"{topic_url}index{page_number}.html\"\n",
    "\n",
    "        print(f\"Navigating to: {current_page_url}\")\n",
    "        scrape_current_page(current_page_url, topic_tag)\n",
    "        page_number += 1\n",
    "\n",
    "    index += 1\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "posts=pd.DataFrame(posts)\n",
    "previousCollected=pd.read_csv(\"../data/healthBordsPosts.csv\")\n",
    "data=pd.concat([posts,previousCollected])\n",
    "data.to_csv(\"../data/healthBordsPosts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting collecting posts Texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "postsDataset=pd.read_csv(\"../data/healthBordsPosts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "posts = []\n",
    "\n",
    "def scrape_current_page(url, topicTag):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"threadslist\")))\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    try:\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post: {e}\")\n",
    "        return False\n",
    "\n",
    "for  index in indexes[2:]:\n",
    "   \n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
